\documentclass[9pt]{extarticle}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{trees}

\title{NEA}
\author{Matthew Lugg}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

Lazy evaluation is a computation model whereby all computations are deferred
until their results are required; this contrasts with eager evaluation, as used
by most conventional imperative languages. Lazy evaluation goes hand-in-hand
with functional programming, a programming paradigm based on Church's lambda
calculus rather than Turing machines. My goal is to write, in Haskell, a simple
compiler for a lazy, purely functional, strongly typed programming language
using the Hindley-Milner type system to amd64 assembly, which will allow for
simple Haskell-like programs to be written and compiled for modern CPUs.

\section{Analysis}

\subsection{Existing solutions}

Haskell is generally considered the de-facto standard for lazy, purely
functional programming languages. It first appeared in 1990, and has been
extended over time to become a highly complex language today. Technically,
Haskell 2010 (the latest version of the language specification) is not
necessarily lazy, but non-strict. However, all popular implementations use lazy
semantics, so for all intents and purposes, Haskell can be considered a lazy
language. The most popular implementation is GHC, the Glasgow Haskell Compiler;
this compiler generates incredibly fast output assembly (often comparable in
speed to programs written in low-level languages like C) and supports over 100
language extensions for features such as GADTs and functional dependencies. 

There exist several other major purely functional languages, some of which
feature laziness. A prominent example is OCaml; however, this language differs
from Haskell in that it is eagerly evaluated by default and has opt-in lazy
semantics. Haskell is somewhat unique; as such, it may prove to be a major
source of inspiration for this project as the most similar existing solution. 

GHC's dialect of Haskell contains various sophisticated features with varying
levels of usage. Haskell, since its inception, contains typeclasses as a major
feature; these aid in the creation of polymorphic functions for things like
sorting. This is one of Haskell's most well-known and commonly-used features,
and as such, I would like to support this in my implementation; however, time
constraints will likely make this infeasible, as type inference and checking of
typeclasses is a relatively complex feature. 

\subsection{Technical overview}

\subsubsection{Parsing}

Parsing is an important part of any compiler; it is the translation of the
source code into a datatype known as an abstract syntax tree (AST); source code
is sometimes first converted into a ``concrete syntax tree'' instead, and then
into an AST, but this is relatively rare. As an example, in a conventional
imperative language, the expression \verb'f(x + y)' would parse into a tree like the
following:

\vspace{0.5cm}

\begin{center}
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=3cm},
    level 2/.style={sibling distance=3cm}]
  \node{\texttt{FnCall}}
    child {node {\texttt{Name "f"}}}
    child {node {\texttt{Add}}
      child {node {\texttt{Name "x"}}}
      child {node {\texttt{Name "y"}}}
    };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}

There are several different techniques of parsing, such as recursive descent and
LL. These methods often suffer from complex, unreadable code. However, Haskell's
type system allows me to use a different approach for parsing: parser
combinators. These allow you to use operators to combine simple parsers in
different ways to create complex ones. This technique of parsing is not without
issues - one of the main ones is that it is quite slow compared to conventional
techniques like recursive descent. However, the speed is still acceptable, and
their simplicity means that this is likely the technique I will use for parsing
my language.

The syntax of purely functional programming languages tends to be relatively
similar; they are mostly based off of either Haskell or Standard ML.
Haskell-like syntax looks like this:

\begin{verbatim}
  -- This is a comment
  foo :: Int -> Int  -- The type of "foo": it is a function from int to int
  foo x = x + 2      -- The definition of "foo": it adds 2 to its argument
\end{verbatim}

This is a very readable syntax, and has inspired other languages like OCaml.
However, it uses indentation for parsing; this can be quite complicated to do.
As such, for simplicity, I plan to change this syntax style slightly to allow
parsing without indentation. On the downside, this will make the syntax slightly
harder to read; however, it will simplify the parser greatly.

\subsubsection{Data types}

A more simple feature of Haskell is algebraic data types (ADTs). These are
datastructures which have multiple constructors, each containing some number of
values; for instance, a data type describing a list of integers could be
recursively defined in Haskell as follows: 

\begin{verbatim}
  data IntList = Cons Int IntList | Nil 
\end{verbatim}

This declaration specifies that an IntList either consists of the Cons
constructor applied to an Int and another IntList (representing the rest of the
list), or the Nil constructor applied to no arguments, which represents the
empty list. This feature is shared by non-lazy languages like OCaml and Standard
ML; it allows for a simple, intuitive, and safe representation of many data
structures. 

\subsubsection{Lazy evaluation}

Laziness is a relatively broad term in programming langauges; language like
Haskell provide purely lazy semantics, while some like OCaml allow you to opt-in
to laziness, and others like Perl 6 support laziness only in instances of
certain data structures (like lists). Here, we will be looking at a fully lazily
evaluated language, as in Haskell. This form of laziness can be incredibly
useful for defining recursive datastructures; for instance, the definition of an
infinite list in Haskell is simply as follows: 

\begin{verbatim}
  x = 1 : x 
\end{verbatim}

This definition works because the elements of x are only evaluated when they
need to be. 

\subsubsection{Type systems}

Another major feature of most functional languages is the type system.
Sophisticated type systems allow the programmer to specify very precise types
for values in the language; they act as a form of machine-checked documentation,
but also come with benefits like easy polymorphism.

Most ``interesting'' type systems are based on the lambda calculus, a theoretical
model for program execution (like Turing machines). There are three main type
systems to consider; the simply typed lambda calculus (hereafter referred to as
STLC for brevity), the Hindley-Milner type system, and System F.

\paragraph{Simply-typed lambda calculus}

STLC is, as the name suggests, effectively the most simple possible type
system for the lambda calculus. It allows polymorphic types to be
constructed using function arrows, written \verb'->', and named type
variables. For example, the term \verb'\x. x' could be ascribed the type
\verb'a -> a', representing a function from a value of some type
\verb'a' to that same type. Similarly, \verb'\f. \x. f x' could be
ascribed the more complex type \verb'(a -> b) -> a -> b', representing a
function which converts a function \verb'a -> b' and a value \verb'a'
into a value \verb'b'. This is an incredibly helpful theoretical model;
however, it is not so useful in practical programming languages, since
for both efficiency and ease of code we want many, many more types than
just functions. Additionally, the simply-typed lambda calculus is
Turing-incomplete, making it impractical for use in a programming
language.

\paragraph{Hindley-Milner typing}

The Hindley-Milner type system (sometimes referred to as Damas-Milner) extends
the STLC with the ability to work with types other than functions. This
is an incredibly popular type system for several reasons; the main one
is that type inference is completely decidable within this system.  This
means that given an expression with no type annotations (explicit pieces
of code informing the compiler of the type of expressions), the compiler
can always infer the most general (i.e.  polymorphic) type for an
expression by itself. This type system is used by the Haskell spec,
whose type system is effectively Hindley-Milner extended with
typeclasses. There also exists a relatively straightforward algorithm
for describing this type system, called Algorithm W. HM is proven to be
effectively as expressive as it is possible for a type system to be
whilst retaining decidability.

\paragraph{System F}

System F is a more sophisticated type system, implemented by modern GHC's
dialect of Haskell. It extends the Hindley-Milner system with rank N types - in
layman's terms, ``nested polymorphism''. For instance:

\begin{verbatim}
  \f. (f 3, f "hi")
\end{verbatim}

This is a lambda which applies a function, given as its argument, to the values
3 and ``hi''. Because these values have different types, this program cannot be
expressed in the Hindley-Milner type system. These more complex types can be
beneficial in several areas of code (hence GHC's support for them); however,
they also greatly increase the complexity of the type system. Type inference
within System F is undecidable; this means it requires user-supplied type
annotations for inference to be successful. There is also no standard algorithm
for type checking and inference of System F expressions; there are many
competing methods of doing this, such as bidirectional type systems (e.g. the
one described in ``Practical type inference for arbitrary-rank types''; S. Peyton
Jones, D. Vytiniotis, S. Weirich, M. Shields; Microsoft Research). Due to this
complexity, and the relatively few benefits, I will likely not implement the
System F type system in any capacity; instead, I plan to implement the
Hindley-Milner system.

\paragraph{Typeclasses}

Typeclasses are an extention to the Hindley-Milner (or System F) type system
which allow for increased polymorphism. An example of the motivation for these
is a sorting function; a comparison-based sorting function, under simple System
F, cannot be polymorphic. This is because it can not be applied to any type, but
rather only those which are ordered. Typeclasses are effectively a more powerful
version of interfaces as seen in OOP, and are Haskell's defining feature.
However, these complicate type inference and checking drastically, so while they
are a very helpful feature, they are not going to be a primary aim for this
project (although I may revisit them if I have time).

\subsubsection{Lazy evaluation}

In order to understand the implementation of laziness, we must first define what
``evaluation'' of an expression means. To do this, we define three ``forms'' of
expression; normal form (NF), head normal form (HNF), and weak head normal form
(WHNF). 

\begin{itemize}

  \item An expression in normal form is effectively fully evaluated. More
  specifically, an expression in NF is either a data constructor applied to
  arguments all in NF, or a lambda abstraction whose body is also in normal
  form. 

  \item An expression in head normal form is either a data constructor applied to
  arguments (which may be in any form), or a lambda abstraction whose body is
  in head normal form. 

  \item An expression in WHNF is either a data constructor applied to arguments
  (which may be in any form), or a lambda abstraction (whose body may be in
  any form). 

\end{itemize}

The most important of these ``forms'' is weak head normal form (WHNF). The way
evaluation actually occurs in a lazy language is through pattern matching, which
effectively allows you to deconstruct a type. For example, using the previously
defined IntList: 

\begin{verbatim}
  sum = \x -> case x of { Cons x xs -> x + sum xs; Nil -> 0 } 
\end{verbatim}

This defines 'sum' as a lambda (anonymous function) of one argument x. This
argument (which should be an IntList) is then deconstructed with a 'case'
expression; this looks at the constructor of x, and runs a different piece of
code depending on the constructor used. This case expression is the only way
evaluation occurs in most lazy languages. For this, we evaluate the expression
(which may be an unevaluated expression, known as a 'thunk') into weak head
normal form. This guarantees that, as long as the value is of the correct type,
the constructor in question will be evaluated, and can be matched against. 

\subsubsection{Garbage collection}

Like most high-level languages, this language will have automatic memory
management. Therefore, I will have to implement a garbage collector
(GC). There are several common types of garbage collector.

\paragraph{Reference counting}

Reference counting (refcounting) is one of the simplest methods of
garbage collection. In this method, every object managed by the GC has a
field in its header specifying how many other objects reference it (i.e.
have a pointer to it). Whenever another object makes a reference to this
object, its reference count is incremented. When a reference is deleted,
the reference count is decremented. After this decrement, it is also
checked whether the reference count has reached zero; if it has, the
object is freed.

This method of garbage collection has some advantages. One is that it
does not ``stop the world''; this means that, as oppose to most other
methods, the program never has to pause execution for an extended period
of time to allow the garbage collector to run. There is one exception to
this rule however, and it is one of the issues with refcounting;
occasionally, freeing one object can recursively remove many references
and free tens or hundreds of objects, resulting in a noticeable pause.
Refcounting is incredibly simple to implement. However, it has one more
major flaw: it fails to ever clean up any kind of recursive
datastructure. Due to immutability, this may not actually be a problem
in an eager purely functional language; however, lazy semantics allow us
to easily define recursive or mutually recursive datastructures. For
instance, a definition like \verb'x = 1 : x' defines an infinitely long
list of \verb'1's, and because it references itself (hence always has a
refcount of at least one), it would never be cleaned up, which could
result in a memory leak. For this reason, refcounting is rarely used as
a garbage collection method (at least, not in isolation).

\paragraph{Mark-and-sweep}

Mark-and-sweep garbage collectors are one of the most common designs for
GCs, used by languages such as Java and Go. In this design, at certain
times (generally when the heap reaches capacity), execution is paused
(this is a ``stop-the-world'' design), and garbage collection begins.
The garbage collector looks at all objects accessible from the current
scope of execution, and sets a special bit in the header to mark the
object as accessible. It then recursively does the same to all objects
referenced by these objects, and all objects referenced by those, etc.
It repeats this entire process for every stack frame and, at the end, it
has marked every ``active'' object (i.e. one which is accessible by
code). Finally, it iterates over all the allocated objects; any that are
not marked are freed.

This type of garbage collector does not have any issues with recursive
data structures; it will always be able to free as many objects as
possible. However, it can be quite slow; this can be seen in many larger
Java applications, which can have noticable pauses of around a second if
a lot of garbage is being created.

\paragraph{Generational}

A generational garbage collector allocates a small ``nursery'' of memory
where new allocations will occur, normally consisting of aroun 1MiB of
memory. Allocations occur sequentially in this nursery, until there is
not enough space remaining for a required allocation. At this point,
``minor GC'' is triggered. The garbage collector then does a
mark-and-sweep style check of all the objects on this nursery; it finds
those which are accessible by the program. However, instead of just
marking the objects as accessible, it copies them to a new
``generation'' on the heap (this makes it a ``copying GC'') and updates
all references to the object to the new pointer. Objects that are
inaccessible are not copied. After this process is complete, every
object in the nursery has either been found to be accessible, and copied
to the heap, or found to be inaccessible, and ignored. Therefore, no
object is stored in the nursery; so program execution can resume,
reusing the nursery for new allocations. Of course, occasionally, you
also need to perform garbage collection on the larger ``generation'';
this is called major GC. There are many variations on the basic design,
but the broad idea is to have separate regions of memory for different
ages of objects.

This can be a very powerful technique in purely functional languages.
This is due to immutability; because objects are immutable, it is
impossible for an object in a younger generation to be referred to by an
object in an older generation. This makes identifying referenced objects
faster and more efficient. As such, GHC generally employs a generational
approach to garbage collection. Recently, it has been updated to also
optionally use a more complex hybrid approach with mark-and-sweep which
can be more suitable for latency-sensitive programs. I will likely
implement a simple moving generational garbage collector, as it provides
a nice balance between efficiency and simplicity of implementation.

\subsection{Objectives}

\begin{itemize}

  \item Create a parser for a Haskell-like functional programming syntax into an AST

  \item Implement inference and checking for the Hindley-Milner type system with ADTs
  using Algorithm W

  \item Convert this AST into a simpler, untyped IR language

  \item Convert this IR into AMD64 assembly (which supports lazy evaluation with
    thunks), using a low-level runtime written in C

  \item Implement a garbage collector to prevent memory leaks in written code

\end{itemize}

\end{document}
