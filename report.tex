\documentclass[9pt]{extarticle}

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{trees}

\newunicodechar{⊥}{$\bot$}
\DeclareUnicodeCharacter{22A5}{$\bot$}
\DeclareUnicodeCharacter{03BB}{$\lambda$}

\title{NEA}
\author{Matthew Lugg}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

Lazy evaluation is a computation model whereby all computations are deferred
until their results are required; this contrasts with eager evaluation, as used
by most conventional imperative languages. Lazy evaluation goes hand-in-hand
with functional programming, a programming paradigm based on Church's lambda
calculus rather than Turing machines. My goal is to write, in Haskell, a simple
compiler for a lazy, purely functional, strongly typed programming language
using the Hindley-Milner type system to amd64 assembly, which will allow for
simple Haskell-like programs to be written and compiled for modern CPUs.

\section{Analysis}

\subsection{Existing solutions}

Haskell is generally considered the de-facto standard for lazy, purely
functional programming languages. It first appeared in 1990, and has been
extended over time to become a highly complex language today. Technically,
Haskell 2010 (the latest version of the language specification) is not
necessarily lazy, but non-strict. However, all popular implementations use lazy
semantics, so for all intents and purposes, Haskell can be considered a lazy
language. The most popular implementation is GHC, the Glasgow Haskell Compiler;
this compiler generates incredibly fast output assembly (often comparable in
speed to programs written in low-level languages like C) and supports over 100
language extensions for features such as GADTs and functional dependencies.

There exist several other major purely functional languages, some of which
feature laziness. A prominent example is OCaml; however, this language differs
from Haskell in that it is eagerly evaluated by default and has opt-in lazy
semantics. Haskell is somewhat unique; as such, it may prove to be a major
source of inspiration for this project as the most similar existing solution.

GHC's dialect of Haskell contains various sophisticated features with varying
levels of usage. Haskell, since its inception, contains typeclasses as a major
feature; these aid in the creation of polymorphic functions for things like
sorting. This is one of Haskell's most well-known and commonly-used features,
and as such, I would like to support this in my implementation; however, time
constraints will likely make this infeasible, as type inference and checking of
typeclasses is a relatively complex feature.

\subsection{Technical overview}

\subsubsection{Parsing}

Parsing is an important part of any compiler; it is the translation of the
source code into a datatype known as an abstract syntax tree (AST); source code
is sometimes first converted into a ``concrete syntax tree'' instead, and then
into an AST, but this is relatively rare. As an example, in a conventional
imperative language, the expression \verb'f(x + y)' would parse into a tree like the
following:

\vspace{0.5cm}

\begin{center}
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=3cm},
    level 2/.style={sibling distance=3cm}]
  \node{\texttt{FnCall}}
    child {node {\texttt{Name "f"}}}
    child {node {\texttt{Add}}
      child {node {\texttt{Name "x"}}}
      child {node {\texttt{Name "y"}}}
    };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}

There are several different techniques of parsing, such as recursive descent and
LL. These methods often suffer from complex, unreadable code. However, Haskell's
type system allows me to use a different approach for parsing: parser
combinators. These allow you to use operators to combine simple parsers in
different ways to create complex ones. This technique of parsing is not without
issues - one of the main ones is that it is quite slow compared to conventional
techniques like recursive descent. However, the speed is still acceptable, and
their simplicity means that this is likely the technique I will use for parsing
my language.

\paragraph{Syntax inspiration}

The syntax of purely functional programming languages tends to be relatively
similar; they are mostly based off of either Haskell or Standard ML.
Haskell-like syntax looks like this:

\begin{verbatim}
  -- This is a comment
  foo :: Int -> Int  -- The type of "foo": it is a function from int to int
  foo x = x + 2      -- The definition of "foo": it adds 2 to its argument
\end{verbatim}

This is a very readable syntax, and has inspired other languages like OCaml.
However, it uses indentation for parsing; this can be quite complicated to do.
As such, for simplicity, I plan to change this syntax style slightly to allow
parsing without indentation. On the downside, this will make the syntax slightly
harder to read; however, it will simplify the parser greatly.

\paragraph{Language syntax}

My language's syntax will be approximately as follows:

\begin{verbatim}
  -- Line comments are started with two dashes

  {-
    Block comments are delimited like this
    {- They can also be nested -}
  -}

  -- Type signatures and bindings. The semicolon indicates the end of a
  -- top-level expression
  foo :: Int;
  foo = x;

  -- The following is the expression syntax.

  -- Function application (a function 'f' applied to a parameter 'x')
  f x

  -- Lambda abstractions (with a parameter 'x')
  λx -> e
  \x -> e

  -- let-bindings use semicolon-separated bindings enclosed in braces.
  -- The last binding may be terminated with an optional semicolon
  let { x = e0; y = e1 } in e2

  -- case expressions use semicolon-separated cases enclosed in braces.
  -- The last case may be terminated with an optional semicolon
  case { Cons x xs -> xs; Nil -> Nil }
\end{verbatim}

\subsubsection{Data types}

A more simple feature of Haskell is algebraic data types (ADTs). These are
datastructures which have multiple constructors, each containing some number of
values; for instance, a data type describing a list of integers could be
recursively defined in Haskell as follows:

\begin{verbatim}
  data IntList = Cons Int IntList | Nil
\end{verbatim}

This declaration specifies that an \verb'IntList' either consists of the
\verb'Cons' constructor applied to an \verb'Int' and another
\verb'IntList' (representing the rest of the list), or the \verb'Nil'
constructor applied to no arguments, which represents the empty list.
These constructors can be thought of as functions that just store their
arguments (and allow you to later extract them). This feature is shared
by non-lazy languages like OCaml and Standard ML; it allows for a
simple, intuitive, and safe representation of many data structures.

These types can also be polymorphic. For instance, the above
\verb'IntList' type can be generalised to any type as follows:

\begin{verbatim}
  data List a = Cons a (List a) | Nil
\end{verbatim}

\subsubsection{Type systems}

Another major feature of most functional languages is the type system.
Sophisticated type systems allow the programmer to specify very precise types
for values in the language; they act as a form of machine-checked documentation,
but also come with benefits like easy polymorphism.

Most ``interesting'' type systems are based on the lambda calculus, a theoretical
model for program execution (like Turing machines). There are three main type
systems to consider; the simply typed lambda calculus (hereafter referred to as
STLC for brevity), the Hindley-Milner type system, and System F.

\paragraph{Simply-typed lambda calculus}

STLC is, as the name suggests, effectively the most simple possible type
system for the lambda calculus. It allows polymorphic types to be
constructed using function arrows, written \verb'->', and named type
variables. For example, the term \verb'\x. x' could be ascribed the type
\verb'a -> a', representing a function from a value of some type
\verb'a' to that same type. Similarly, \verb'\f. \x. f x' could be
ascribed the more complex type \verb'(a -> b) -> a -> b', representing a
function which converts a function \verb'a -> b' and a value \verb'a'
into a value \verb'b'. This is an incredibly helpful theoretical model;
however, it is not so useful in practical programming languages, since
for both efficiency and ease of code we want many, many more types than
just functions. Additionally, the simply-typed lambda calculus is
Turing-incomplete, making it impractical for use in a programming
language.

\paragraph{Hindley-Milner typing}

The Hindley-Milner type system (sometimes referred to as Damas-Milner) extends
the STLC with the ability to work with types other than functions. This
is an incredibly popular type system for several reasons; the main one
is that type inference is completely decidable within this system.  This
means that given an expression with no type annotations (explicit pieces
of code informing the compiler of the type of expressions), the compiler
can always infer the most general (i.e.  polymorphic) type for an
expression by itself. This type system is used by the Haskell spec,
whose type system is effectively Hindley-Milner extended with
typeclasses. There also exists a relatively straightforward algorithm
for describing this type system, called Algorithm W. HM is proven to be
effectively as expressive as it is possible for a type system to be
whilst retaining decidability.

\paragraph{System F}

System F is a more sophisticated type system, implemented by modern GHC's
dialect of Haskell. It extends the Hindley-Milner system with rank N types - in
layman's terms, ``nested polymorphism''. For instance:

\begin{verbatim}
  \f. (f 3, f "hi")
\end{verbatim}

This is a lambda which applies a function, given as its argument, to the values
3 and ``hi''. Because these values have different types, this program cannot be
expressed in the Hindley-Milner type system. These more complex types can be
beneficial in several areas of code (hence GHC's support for them); however,
they also greatly increase the complexity of the type system. Type inference
within System F is undecidable; this means it requires user-supplied type
annotations for inference to be successful. There is also no standard algorithm
for type checking and inference of System F expressions; there are many
competing methods of doing this, such as bidirectional type systems (e.g. the
one described in ``Practical type inference for arbitrary-rank types''; S. Peyton
Jones, D. Vytiniotis, S. Weirich, M. Shields; Microsoft Research). Due to this
complexity, and the relatively few benefits, I will likely not implement the
System F type system in any capacity; instead, I plan to implement the
Hindley-Milner system.

\paragraph{Typeclasses}

Typeclasses are an extention to the Hindley-Milner (or System F) type system
which allow for increased polymorphism. An example of the motivation for these
is a sorting function; a comparison-based sorting function, under simple System
F, cannot be polymorphic. This is because it can not be applied to any type, but
rather only those which are ordered. Typeclasses are effectively a more powerful
version of interfaces as seen in OOP, and are Haskell's defining feature.
However, these complicate type inference and checking drastically, so while they
are a very helpful feature, they are not going to be a primary aim for this
project (although I may revisit them if I have time).

\subsubsection{Lazy evaluation}

Lazy evaluation is a feature of some programming languages where an
expression is not evaluated until its value is required. For instance,
in the following function:

\begin{verbatim}
  const :: a -> b -> a
  const x y = x
\end{verbatim}

The value of \verb'y' will never be evaluated when \verb'const' is
called. This means a call like the following...

\begin{verbatim}
  const 10 (factorial 100000000)
\end{verbatim}

...will not perform the expensive \verb'factorial' computation. This
language feature becomes useful in writing clean code. As an example, if
a program needs to generate an unkown amount of prime numbers, in an
imperative language you would have to write code which generated extra
ones as they were required. However, in a lazy language, you can simply
define an infinitely long list of prime numbers, and they will only be
computed as they are requested. This allows you to easily separate
production and consumption of data, making it easy to write clean code.

\begin{verbatim}
  -- A naive implementation of an infinite list of primes
  primes :: [Integer]
  primes = filter isPrime [2..]
\end{verbatim}

Laziness is a relatively broad term in programming langauges; languages like
Haskell provide purely lazy semantics, while some like OCaml allow you to opt-in
to laziness, and others like Perl 6 support laziness only in instances of
certain data structures (like lists). Here, we will be looking at a fully lazily
evaluated language, as in Haskell. This form of laziness can be incredibly
useful for defining recursive datastructures; for instance, the definition of an
infinite list in Haskell is simply as follows:

\begin{verbatim}
  x = 1 : x
\end{verbatim}

This definition works because the elements of x are only evaluated when they
need to be.

\paragraph{Bottom values}

To understand the basis for lazy evalution, we first need to discuss
bottom values.

In any sufficiently expressive (Turing-complete) language, you can write
non-total expressions: this means an expression that does not produce a
value. Examples of this include throwing exceptions, looping infinitely,
or recursing infinitely (by the undecidability of the Halting problem,
it is impossible to eliminate such programs computationally). We want to
be able to logically reason about these computations. As such, we say
that these non-terminating or erroring computations are represented by a
special value, ``bottom'', which we generally write as ⊥ or \verb'_|_'.
This value is incredibly special, as it is the only value which inhabits
all types (that is, ⊥ is a valid value of every type).

\begin{verbatim}
  f x = f (x + 1)  -- Calls to this function will never terminate
  f 3 = ⊥   -- Therefore, we can semantically say it returns ⊥
\end{verbatim}

\paragraph{Strict semantics}

Technically speaking, Haskell is not, by specification, a lazily
evaluated language; instead, it is non-strict. Non-strict means that
evaluation proceeds from the outside of an expression inwards, as
opposed to the other way round. The practical meaning of this is to do
with how bottom values propagate throughout expressions. In a strictly
evaluated language - such as most imperative languages - a bottom value
appearing in any subexpression will result in the entire expression
evaluating to bottom; for instance, a function call \verb'f(1, x)' would
neccesarily evalute to ⊥ if \verb'x' is ⊥. Non-strict
languages, on the other hand, could have \verb'f' return a perfectly
valid value. Strict languages often have a small amount of non-strict
behaviour in the form of boolean short-circuiting: in languages like C,
evaluating \verb'true || f(0)' will never call \verb'f', as it is not
neccesary to know that the entire statement returns \verb'true'.
Non-strict languages extend this to all aspects of the language's
syntax.

In practice, most, if not all, implementations of Haskell use laziness
as the method for implementing non-strictness. With lazy evaluation,
function application, rather than actually running code, creates a
datastructure called a ``thunk'', which describes the function and its
argument. Then, when this value is ``forced'' (its evaluation is
required), the function is actually called to calculate the result.

\paragraph{Evaluation}

In order to understand the implementation of laziness, we must first define what
``evaluation'' of an expression means. To do this, we define three ``forms'' of
expression; normal form (NF), head normal form (HNF), and weak head normal form
(WHNF).

\begin{itemize}

  \item An expression in normal form is effectively fully evaluated. More
  specifically, an expression in NF is either a data constructor applied to
  arguments all in NF, or a lambda abstraction whose body is also in NF.

  \item An expression in head normal form is either a data constructor applied to
  arguments (which may be in any form), or a lambda abstraction whose body is
  in head normal form.

  \item An expression in WHNF is either a data constructor applied to arguments
  (which may be in any form), or a lambda abstraction (whose body may be in
  any form).

\end{itemize}

The most important of these ``forms'' is weak head normal form (WHNF).
The way evaluation actually normally occurs in a lazy language is
through pattern matching, which effectively allows you to deconstruct a
type. For example, using the previously defined \verb'IntList':

\begin{verbatim}
  sum = \x -> case x of { Cons x xs -> x + sum xs; Nil -> 0 }
\end{verbatim}

This defines 'sum' as a lambda (anonymous function) of one argument
\verb'x'.  This argument (which should be an \verb'IntList') is then
deconstructed with a \verb'case' expression; this looks at the
constructor of x, and runs a different piece of code depending on the
constructor used. This case expression is the only way evaluation occurs
in most lazy languages. For this, we evaluate the expression (which may
be a thunk) into weak head normal form. This guarantees that, as long as
the value is of the correct type, the constructor in question will be
evaluated, and can be matched against.

\subsubsection{Garbage collection}

Like most high-level languages, this language will have automatic memory
management. Therefore, I will have to implement a garbage collector
(GC). There are several common types of garbage collector.

\paragraph{Reference counting}

Reference counting (refcounting) is one of the simplest methods of
garbage collection. In this method, every object managed by the GC has a
field in its header specifying how many other objects reference it (i.e.
have a pointer to it). Whenever another object makes a reference to this
object, its reference count is incremented. When a reference is deleted,
the reference count is decremented. After this decrement, it is also
checked whether the reference count has reached zero; if it has, the
object is freed.

This method of garbage collection has some advantages. One is that it
does not ``stop the world''; this means that, as oppose to most other
methods, the program never has to pause execution for an extended period
of time to allow the garbage collector to run. There is one exception to
this rule however, and it is one of the issues with refcounting;
occasionally, freeing one object can recursively remove many references
and free tens or hundreds of objects, resulting in a noticeable pause.
Refcounting is incredibly simple to implement. However, it has one more
major flaw: it fails to ever clean up any kind of recursive
datastructure. Due to immutability, this may not actually be a problem
in an eager purely functional language; however, lazy semantics allow us
to easily define recursive or mutually recursive datastructures. For
instance, a definition like \verb'x = 1 : x' defines an infinitely long
list of \verb'1's, and because it references itself (hence always has a
refcount of at least one), it would never be cleaned up, which could
result in a memory leak. For this reason, refcounting is rarely used as
a garbage collection method (at least, not in isolation).

\paragraph{Mark-and-sweep}

Mark-and-sweep garbage collectors are one of the most common designs for
GCs, used by languages such as Java and Go. In this design, at certain
times (generally when the heap reaches capacity), execution is paused
(this is a ``stop-the-world'' design), and garbage collection begins.
The garbage collector looks at all objects accessible from the current
scope of execution, and sets a special bit in the header to mark the
object as accessible. It then recursively does the same to all objects
referenced by these objects, and all objects referenced by those, etc.
It repeats this entire process for every stack frame and, at the end, it
has marked every ``active'' object (i.e. one which is accessible by
code). Finally, it iterates over all the allocated objects; any that are
not marked are freed.

This type of garbage collector does not have any issues with recursive
data structures; it will always be able to free as many objects as
possible. However, it can be quite slow; this can be seen in many larger
Java applications, which can have noticable pauses of around a second if
a lot of garbage is being created.

\paragraph{Generational}

A generational garbage collector allocates a small ``nursery'' of memory
where new allocations will occur, normally consisting of aroun 1MiB of
memory. Allocations occur sequentially in this nursery, until there is
not enough space remaining for a required allocation. At this point,
``minor GC'' is triggered. The garbage collector then does a
mark-and-sweep style check of all the objects on this nursery; it finds
those which are accessible by the program. However, instead of just
marking the objects as accessible, it copies them to a new
``generation'' on the heap (this makes it a ``copying GC'') and updates
all references to the object to the new pointer. Objects that are
inaccessible are not copied. After this process is complete, every
object in the nursery has either been found to be accessible, and copied
to the heap, or found to be inaccessible, and ignored. Therefore, no
object is stored in the nursery; so program execution can resume,
reusing the nursery for new allocations. Of course, occasionally, you
also need to perform garbage collection on the larger ``generation'';
this is called major GC. There are many variations on the basic design,
but the broad idea is to have separate regions of memory for different
ages of objects.

This can be a very powerful technique in purely functional languages.
This is due to immutability; because objects are immutable, it is
impossible for an object in a younger generation to be referred to by an
object in an older generation. This makes identifying referenced objects
faster and more efficient. As such, GHC generally employs a generational
approach to garbage collection. Recently, it has been updated to also
optionally use a more complex hybrid approach with mark-and-sweep which
can be more suitable for latency-sensitive programs. I will likely
implement a simple moving generational garbage collector, as it provides
a nice balance between efficiency and simplicity of implementation.

\subsection{Intermediate representations}

Compiling a high-level language like this to assembly is a complex task.
For this reason, compilers do not typically translate source directly;
instead, they move through one or more ``intermediate representations''
(IRs), each lower-level than the last. These are datastructures very
similar to an AST, with the difference that they do not correspond to
any literal source language. GHC uses three IRs: Core, STG and Cmm.

Core is GHC's first IR, and its most well-known. It is an incredibly
simple functional IR; similar to Haskell itself but much smaller.
Expressions in Core are defined as an ADT with the following 10
constructors:

\begin{verbatim}
  data Expr b    -- "b" for the type of binders
    = Var   Id
    | Lit   Literal
    | App   (Expr b) (Arg b)
    | Lam   b (Expr b)
    | Let   (Bind b) (Expr b)
    | Case  (Expr b) b Type [Alt b]
    | Cast  (Expr b) Coercion
    | Tick  (Tickish Id) (Expr b)
    | Type  Type
    | Coercion Coercion
\end{verbatim}

However, several parts of this type are not relevant - \verb'Tick'
relates to debugging information, and \verb'Coercion', \verb'Type' and
\verb'Cast' are concerned with more complex features of GHC like
functional dependencies and type families. A simplified version of Core
is as follows:

\begin{verbatim}
  data Expr b    -- "b" for the type of binders
    = Var   Id
    | Lit   Literal
    | App   (Expr b) (Expr b)
    | Lam   b (Expr b)
    | Let   (Bind b) (Expr b)
    | Case  (Expr b) b Type [Alt b]
\end{verbatim}

This tells us that a Core expression is either a variable, a literal, a
function applied to an argument, a lambda abstraction, a set of let
bindings, or a case expression. Notice that these expressions are
parameterized over a type \verb'b', describing bound variables. This is
used to add extra type information to the IR. The IR does not strictly
need much type information, as type checking in GHC occurs on the
original source language. However, some type information is important
later in compilation, and preserving it can make it easier to find and
fix compiler bugs when they arise.

I will likely use a Core-like IR in compilation of my language. This is
because it allows you to remove any syntactic sugar in the language, and
generally simplify the representation, before proceeding with
compilation. There will be two main differences between my IR and my
source language.

The first is in let bindings. Before type checking, let bindings are
split into ``binding groups''; groups of mutually recursive bindings.
TODO why are they in core?

The second is in case expressions. In the source language, pattern
matches may be nested arbitrarily:

\begin{verbatim}
  case xs of
    { Cons p (Cons q (Cons r Nil)) -> e0
    ; _ -> e1 } -- default case
\end{verbatim}

However, this cannot be directly compiled. Instead, the cases must be
flattened into matching single constructors:

\begin{verbatim}
  case xs of
    { Cons p tmp0 ->
      case tmp0 of
        { Cons q tmp1 ->
          case tmp1 of
            { Cons r tmp2 ->
              case tmp2 of
                { Nil -> e0
                ; _   -> e1 }
            ; _ -> e1 }
        ; _ -> e1 }
    ; _ -> e1 }
\end{verbatim}

For simplicity, my IR will require a default case in every pattern
match. If not provided, this will be automatically generated as an
expression that throws an error.

\subsection{Objectives}

\begin{itemize}

  \item Create a parser for a Haskell-like functional programming syntax into an AST

  \item Implement inference and checking for the Hindley-Milner type system with ADTs
  using Algorithm W

  \item Convert this AST into a simpler, untyped IR language

  \item Convert this IR into AMD64 assembly (which supports lazy evaluation with
    thunks), using a low-level runtime written in C

  \item Implement a garbage collector to prevent memory leaks in written code

\end{itemize}

\end{document}
