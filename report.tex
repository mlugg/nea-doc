\documentclass[9pt]{extarticle}

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{trees}

\newunicodechar{⊥}{$\bot$}
\DeclareUnicodeCharacter{22A5}{$\bot$}
\DeclareUnicodeCharacter{03BB}{$\lambda$}

\title{NEA}
\author{Matthew Lugg}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

Lazy evaluation is a computation model whereby all computations are deferred
until their results are required; this contrasts with eager evaluation, as used
by most conventional imperative languages. Lazy evaluation goes hand-in-hand
with functional programming, a programming paradigm based on Church's lambda
calculus rather than Turing machines. My goal is to write, in Haskell, a simple
compiler for a lazy, purely functional, strongly typed programming language
using the Hindley-Milner type system to amd64 assembly, which will allow for
simple Haskell-like programs to be written and compiled for modern CPUs.

\section{Analysis}

\subsection{Existing solutions}

Haskell is generally considered the de-facto standard for lazy, purely
functional programming languages. It first appeared in 1990, and has been
extended over time to become a highly complex language today. Technically,
Haskell 2010 (the latest version of the language specification) is not
necessarily lazy, but non-strict. However, all popular implementations use lazy
semantics, so for all intents and purposes, Haskell can be considered a lazy
language. The most popular implementation is GHC, the Glasgow Haskell Compiler;
this compiler generates incredibly fast output assembly (often comparable in
speed to programs written in low-level languages like C) and supports over 100
language extensions for features such as GADTs and functional dependencies.

There exist several other major purely functional languages, some of which
feature laziness. A prominent example is OCaml; however, this language differs
from Haskell in that it is eagerly evaluated by default and has opt-in lazy
semantics. Haskell is somewhat unique; as such, it may prove to be a major
source of inspiration for this project as the most similar existing solution.

GHC's dialect of Haskell contains various sophisticated features with varying
levels of usage. Haskell, since its inception, contains typeclasses as a major
feature; these aid in the creation of polymorphic functions for things like
sorting. This is one of Haskell's most well-known and commonly-used features,
and as such, I would like to support this in my implementation; however, time
constraints will likely make this infeasible, as type inference and checking of
typeclasses is a relatively complex feature.

\subsection{Technical overview}

\subsubsection{Parsing}

Parsing is an important part of any compiler; it is the translation of the
source code into a datatype known as an abstract syntax tree (AST); source code
is sometimes first converted into a ``concrete syntax tree'' instead, and then
into an AST, but this is relatively rare. As an example, in a conventional
imperative language, the expression \verb'f(x + y)' would parse into a tree like the
following:

\vspace{0.5cm}

\begin{center}
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=3cm},
    level 2/.style={sibling distance=3cm}]
  \node{\texttt{FnCall}}
    child {node {\texttt{Name "f"}}}
    child {node {\texttt{Add}}
      child {node {\texttt{Name "x"}}}
      child {node {\texttt{Name "y"}}}
    };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}

There are several different techniques of parsing, such as recursive descent and
LL. These methods often suffer from complex, unreadable code. However, Haskell's
type system allows me to use a different approach for parsing: parser
combinators. These allow you to use operators to combine simple parsers in
different ways to create complex ones. This technique of parsing is not without
issues - one of the main ones is that it is quite slow compared to conventional
techniques like recursive descent. However, the speed is still acceptable, and
their simplicity means that this is likely the technique I will use for parsing
my language.

\paragraph{Syntax inspiration}

The syntax of purely functional programming languages tends to be relatively
similar; they are mostly based off of either Haskell or Standard ML.
Haskell-like syntax looks like this:

\begin{verbatim}
  -- This is a comment
  foo :: Int -> Int  -- The type of "foo": it is a function from int to int
  foo x = x + 2      -- The definition of "foo": it adds 2 to its argument
\end{verbatim}

This is a very readable syntax, and has inspired other languages like OCaml.
However, it uses indentation for parsing; this can be quite complicated to do.
As such, for simplicity, I plan to change this syntax style slightly to allow
parsing without indentation. On the downside, this will make the syntax slightly
harder to read; however, it will simplify the parser greatly.

\paragraph{Language syntax}

My language's syntax will be approximately as follows:

\begin{verbatim}
  -- Line comments are started with two dashes

  {-
    Block comments are delimited like this
    {- They can also be nested -}
  -}

  -- Type signatures and bindings. The semicolon indicates the end of a
  -- top-level expression
  foo :: Int;
  foo = x;

  -- The following is the expression syntax.

  -- Function application (a function 'f' applied to a parameter 'x')
  f x

  -- Lambda abstractions (with a parameter 'x')
  λx -> e
  \x -> e

  -- let-bindings use semicolon-separated bindings enclosed in braces.
  -- The last binding may be terminated with an optional semicolon
  let { x = e0; y = e1 } in e2

  -- case expressions use semicolon-separated cases enclosed in braces.
  -- The last case may be terminated with an optional semicolon
  case { Cons x xs -> xs; Nil -> Nil }
\end{verbatim}

\subsubsection{Data types}

A more simple feature of Haskell is algebraic data types (ADTs). These are
datastructures which have multiple constructors, each containing some number of
values; for instance, a data type describing a list of integers could be
recursively defined in Haskell as follows:

\begin{verbatim}
  data IntList = Cons Int IntList | Nil
\end{verbatim}

This declaration specifies that an \verb'IntList' either consists of the
\verb'Cons' constructor applied to an \verb'Int' and another
\verb'IntList' (representing the rest of the list), or the \verb'Nil'
constructor applied to no arguments, which represents the empty list.
These constructors can be thought of as functions that just store their
arguments (and allow you to later extract them). This feature is shared
by non-lazy languages like OCaml and Standard ML; it allows for a
simple, intuitive, and safe representation of many data structures.

These types can also be polymorphic. For instance, the above
\verb'IntList' type can be generalised to any type as follows:

\begin{verbatim}
  data List a = Cons a (List a) | Nil
\end{verbatim}

\subsubsection{Type systems}

Another major feature of most functional languages is the type system.
Sophisticated type systems allow the programmer to specify very precise types
for values in the language; they act as a form of machine-checked documentation,
but also come with benefits like easy polymorphism.

Most ``interesting'' type systems are based on the lambda calculus, a theoretical
model for program execution (like Turing machines). There are three main type
systems to consider; the simply typed lambda calculus (hereafter referred to as
STLC for brevity), the Hindley-Milner type system, and System F.

\paragraph{Simply-typed lambda calculus}

STLC is, as the name suggests, effectively the most simple possible type
system for the lambda calculus. It allows polymorphic types to be
constructed using function arrows, written \verb'->', and named type
variables. For example, the term \verb'\x. x' could be ascribed the type
\verb'a -> a', representing a function from a value of some type
\verb'a' to that same type. Similarly, \verb'\f. \x. f x' could be
ascribed the more complex type \verb'(a -> b) -> a -> b', representing a
function which converts a function \verb'a -> b' and a value \verb'a'
into a value \verb'b'. This is an incredibly helpful theoretical model;
however, it is not so useful in practical programming languages, since
for both efficiency and ease of code we want many, many more types than
just functions. Additionally, the simply-typed lambda calculus is
Turing-incomplete, making it impractical for use in a programming
language.

\paragraph{Hindley-Milner typing}

The Hindley-Milner type system (sometimes referred to as Damas-Milner) extends
the STLC with the ability to work with types other than functions. This
is an incredibly popular type system for several reasons; the main one
is that type inference is completely decidable within this system.  This
means that given an expression with no type annotations (explicit pieces
of code informing the compiler of the type of expressions), the compiler
can always infer the most general (i.e.  polymorphic) type for an
expression by itself. This type system is used by the Haskell spec,
whose type system is effectively Hindley-Milner extended with
typeclasses. There also exists a relatively straightforward algorithm
for describing this type system, called Algorithm W. HM is proven to be
effectively as expressive as it is possible for a type system to be
whilst retaining decidability.

\paragraph{System F}

System F is a more sophisticated type system, implemented by modern GHC's
dialect of Haskell. It extends the Hindley-Milner system with rank N types - in
layman's terms, ``nested polymorphism''. For instance:

\begin{verbatim}
  \f. (f 3, f "hi")
\end{verbatim}

This is a lambda which applies a function, given as its argument, to the values
3 and ``hi''. Because these values have different types, this program cannot be
expressed in the Hindley-Milner type system. These more complex types can be
beneficial in several areas of code (hence GHC's support for them); however,
they also greatly increase the complexity of the type system. Type inference
within System F is undecidable; this means it requires user-supplied type
annotations for inference to be successful. There is also no standard algorithm
for type checking and inference of System F expressions; there are many
competing methods of doing this, such as bidirectional type systems (e.g. the
one described in ``Practical type inference for arbitrary-rank types''; S. Peyton
Jones, D. Vytiniotis, S. Weirich, M. Shields; Microsoft Research). Due to this
complexity, and the relatively few benefits, I will likely not implement the
System F type system in any capacity; instead, I plan to implement the
Hindley-Milner system.

\paragraph{Typeclasses}

Typeclasses are an extention to the Hindley-Milner (or System F) type system
which allow for increased polymorphism. An example of the motivation for these
is a sorting function; a comparison-based sorting function, under simple System
F, cannot be polymorphic. This is because it can not be applied to any type, but
rather only those which are ordered. Typeclasses are effectively a more powerful
version of interfaces as seen in OOP, and are Haskell's defining feature.
However, these complicate type inference and checking drastically, so while they
are a very helpful feature, they are not going to be a primary aim for this
project (although I may revisit them if I have time).

\subsubsection{Lazy evaluation}

Lazy evaluation is a feature of some programming languages where an
expression is not evaluated until its value is required. For instance,
in the following function:

\begin{verbatim}
  const :: a -> b -> a
  const x y = x
\end{verbatim}

The value of \verb'y' will never be evaluated when \verb'const' is
called. This means a call like the following...

\begin{verbatim}
  const 10 (factorial 100000000)
\end{verbatim}

...will not perform the expensive \verb'factorial' computation. This
language feature becomes useful in writing clean code. As an example, if
a program needs to generate an unkown amount of prime numbers, in an
imperative language you would have to write code which generated extra
ones as they were required. However, in a lazy language, you can simply
define an infinitely long list of prime numbers, and they will only be
computed as they are requested. This allows you to easily separate
production and consumption of data, making it easy to write clean code.

\begin{verbatim}
  -- A naive implementation of an infinite list of primes
  primes :: [Integer]
  primes = filter isPrime [2..]
\end{verbatim}

Laziness is a relatively broad term in programming langauges; languages like
Haskell provide purely lazy semantics, while some like OCaml allow you to opt-in
to laziness, and others like Perl 6 support laziness only in instances of
certain data structures (like lists). Here, we will be looking at a fully lazily
evaluated language, as in Haskell. This form of laziness can be incredibly
useful for defining recursive datastructures; for instance, the definition of an
infinite list in Haskell is simply as follows:

\begin{verbatim}
  x = 1 : x
\end{verbatim}

This definition works because the elements of x are only evaluated when they
need to be.

\paragraph{Bottom values}

To understand the basis for lazy evalution, we first need to discuss
bottom values.

In any sufficiently expressive (Turing-complete) language, you can write
non-total expressions: this means an expression that does not produce a
value. Examples of this include throwing exceptions, looping infinitely,
or recursing infinitely (by the undecidability of the Halting problem,
it is impossible to eliminate such programs computationally). We want to
be able to logically reason about these computations. As such, we say
that these non-terminating or erroring computations are represented by a
special value, ``bottom'', which we generally write as ⊥ or \verb'_|_'.
This value is incredibly special, as it is the only value which inhabits
all types (that is, ⊥ is a valid value of every type).

\begin{verbatim}
  f x = f (x + 1)  -- Calls to this function will never terminate
  f 3 = ⊥   -- Therefore, we can semantically say it returns ⊥
\end{verbatim}

\paragraph{Strict semantics}

Technically speaking, Haskell is not, by specification, a lazily
evaluated language; instead, it is non-strict. Non-strict means that
evaluation proceeds from the outside of an expression inwards, as
opposed to the other way round. The practical meaning of this is to do
with how bottom values propagate throughout expressions. In a strictly
evaluated language - such as most imperative languages - a bottom value
appearing in any subexpression will result in the entire expression
evaluating to bottom; for instance, a function call \verb'f(1, x)' would
neccesarily evalute to ⊥ if \verb'x' is ⊥. Non-strict
languages, on the other hand, could have \verb'f' return a perfectly
valid value. Strict languages often have a small amount of non-strict
behaviour in the form of boolean short-circuiting: in languages like C,
evaluating \verb'true || f(0)' will never call \verb'f', as it is not
neccesary to know that the entire statement returns \verb'true'.
Non-strict languages extend this to all aspects of the language's
syntax.

In practice, most, if not all, implementations of Haskell use laziness
as the method for implementing non-strictness. With lazy evaluation,
function application, rather than actually running code, creates a
datastructure called a ``thunk'', which describes the function and its
argument. Then, when this value is ``forced'' (its evaluation is
required), the function is actually called to calculate the result.

\paragraph{Evaluation}

In order to understand the implementation of laziness, we must first define what
``evaluation'' of an expression means. To do this, we define three ``forms'' of
expression; normal form (NF), head normal form (HNF), and weak head normal form
(WHNF).

\begin{itemize}

  \item An expression in normal form is effectively fully evaluated. More
  specifically, an expression in NF is either a data constructor applied to
  arguments all in NF, or a lambda abstraction whose body is also in NF.

  \item An expression in head normal form is either a data constructor applied to
  arguments (which may be in any form), or a lambda abstraction whose body is
  in head normal form.

  \item An expression in WHNF is either a data constructor applied to arguments
  (which may be in any form), or a lambda abstraction (whose body may be in
  any form).

\end{itemize}

The most important of these ``forms'' is weak head normal form (WHNF).
The way evaluation actually normally occurs in a lazy language is
through pattern matching, which effectively allows you to deconstruct a
type. For example, using the previously defined \verb'IntList':

\begin{verbatim}
  sum = \x -> case x of { Cons x xs -> x + sum xs; Nil -> 0 }
\end{verbatim}

This defines 'sum' as a lambda (anonymous function) of one argument
\verb'x'.  This argument (which should be an \verb'IntList') is then
deconstructed with a \verb'case' expression; this looks at the
constructor of x, and runs a different piece of code depending on the
constructor used. This case expression is the only way evaluation occurs
in most lazy languages. For this, we evaluate the expression (which may
be a thunk) into weak head normal form. This guarantees that, as long as
the value is of the correct type, the constructor in question will be
evaluated, and can be matched against.

\subsubsection{Garbage collection}

Like most high-level languages, this language will have automatic memory
management. Therefore, I will have to implement a garbage collector
(GC). There are several common types of garbage collector.

\paragraph{Reference counting}

Reference counting (refcounting) is one of the simplest methods of
garbage collection. In this method, every object managed by the GC has a
field in its header specifying how many other objects reference it (i.e.
have a pointer to it). Whenever another object makes a reference to this
object, its reference count is incremented. When a reference is deleted,
the reference count is decremented. After this decrement, it is also
checked whether the reference count has reached zero; if it has, the
object is freed.

This method of garbage collection has some advantages. One is that it
does not ``stop the world''; this means that, as oppose to most other
methods, the program never has to pause execution for an extended period
of time to allow the garbage collector to run. There is one exception to
this rule however, and it is one of the issues with refcounting;
occasionally, freeing one object can recursively remove many references
and free tens or hundreds of objects, resulting in a noticeable pause.
Refcounting is incredibly simple to implement. However, it has one more
major flaw: it fails to ever clean up any kind of recursive
datastructure. Due to immutability, this may not actually be a problem
in an eager purely functional language; however, lazy semantics allow us
to easily define recursive or mutually recursive datastructures. For
instance, a definition like \verb'x = 1 : x' defines an infinitely long
list of \verb'1's, and because it references itself (hence always has a
refcount of at least one), it would never be cleaned up, which could
result in a memory leak. For this reason, refcounting is rarely used as
a garbage collection method (at least, not in isolation).

\paragraph{Mark-and-sweep}

Mark-and-sweep garbage collectors are one of the most common designs for
GCs, used by languages such as Java and Go. In this design, at certain
times (generally when the heap reaches capacity), execution is paused
(this is a ``stop-the-world'' design), and garbage collection begins.
The garbage collector looks at all objects accessible from the current
scope of execution, and sets a special bit in the header to mark the
object as accessible. It then recursively does the same to all objects
referenced by these objects, and all objects referenced by those, etc.
It repeats this entire process for every stack frame and, at the end, it
has marked every ``active'' object (i.e. one which is accessible by
code). Finally, it iterates over all the allocated objects; any that are
not marked are freed.

This type of garbage collector does not have any issues with recursive
data structures; it will always be able to free as many objects as
possible. However, it can be quite slow; this can be seen in many larger
Java applications, which can have noticable pauses of around a second if
a lot of garbage is being created.

\paragraph{Generational}

A generational garbage collector allocates a small ``nursery'' of memory
where new allocations will occur, normally consisting of aroun 1MiB of
memory. Allocations occur sequentially in this nursery, until there is
not enough space remaining for a required allocation. At this point,
``minor GC'' is triggered. The garbage collector then does a
mark-and-sweep style check of all the objects on this nursery; it finds
those which are accessible by the program. However, instead of just
marking the objects as accessible, it copies them to a new
``generation'' on the heap (this makes it a ``copying GC'') and updates
all references to the object to the new pointer. Objects that are
inaccessible are not copied. After this process is complete, every
object in the nursery has either been found to be accessible, and copied
to the heap, or found to be inaccessible, and ignored. Therefore, no
object is stored in the nursery; so program execution can resume,
reusing the nursery for new allocations. Of course, occasionally, you
also need to perform garbage collection on the larger ``generation'';
this is called major GC. There are many variations on the basic design,
but the broad idea is to have separate regions of memory for different
ages of objects.

This can be a very powerful technique in purely functional languages.
This is due to immutability; because objects are immutable, it is
impossible for an object in a younger generation to be referred to by an
object in an older generation. This makes identifying referenced objects
faster and more efficient. As such, GHC generally employs a generational
approach to garbage collection. Recently, it has been updated to also
optionally use a more complex hybrid approach with mark-and-sweep which
can be more suitable for latency-sensitive programs. I will likely
implement a simple moving generational garbage collector, as it provides
a nice balance between efficiency and simplicity of implementation.

\subsubsection{Intermediate representations}

Compiling a high-level language like this to assembly is a complex task.
For this reason, compilers do not typically translate source directly;
instead, they move through one or more ``intermediate representations''
(IRs), each lower-level than the last. These are datastructures very
similar to an AST, with the difference that they do not correspond to
any literal source language. GHC uses three IRs: Core, STG and Cmm.

Core is GHC's first IR, and its most well-known. It is an incredibly
simple functional IR; similar to Haskell itself but much smaller.
Expressions in Core are defined as an ADT with the following 10
constructors:

\begin{verbatim}
  data Expr b    -- "b" for the type of binders
    = Var   Id
    | Lit   Literal
    | App   (Expr b) (Arg b)
    | Lam   b (Expr b)
    | Let   (Bind b) (Expr b)
    | Case  (Expr b) b Type [Alt b]
    | Cast  (Expr b) Coercion
    | Tick  (Tickish Id) (Expr b)
    | Type  Type
    | Coercion Coercion
\end{verbatim}

However, several parts of this type are not relevant - \verb'Tick'
relates to debugging information, and \verb'Coercion' and \verb'Cast'
are concerned with more complex features of GHC like functional
dependencies and type families. A simplified version of Core is as
follows:

\begin{verbatim}
  data Expr b    -- "b" for the type of binders
    = Var   Id
    | Lit   Literal
    | App   (Expr b) (Expr b)
    | Lam   b (Expr b)
    | Let   (Bind b) (Expr b)
    | Case  (Expr b) b Type [Alt b]
    | Type  Type
\end{verbatim}

This tells us that a Core expression is either a variable, a literal, a
function applied to an argument, a lambda abstraction, a set of let
bindings, a case expression, or a type. Types being in this IR seems
strange at first; the reason for this difference is that Core uses the
notion of type lambdas to represent polymorphism. A simple function like
\verb'id :: a -> a' is, in Core, represented as
\verb'id = \(@ t) -> \(x :: t) -> x', where the first parameter is the
type and the second is a value of that type. This facility is not
neccesary for HM-like type systems; it is only present in Core as this
language describes a variant of System F.  Also notice that Core
expressions are parameterized over a type \verb'b', describing bound
variables. This is used to add extra metadata to the IR.

I will likely use a Core-like IR in compilation of my language. This is
because it allows you to remove any syntactic sugar in the language, and
generally simplify the representation, before proceeding with
compilation. There will be two main differences between my IR and my
source language.

The first is in let bindings. Before or during type checking, let
bindings are split into ``binding groups''; groups of mutually recursive
bindings. Keeping this separation makes the IR slightly simpler to
compile later on.

The second is in case expressions. In the source language, pattern
matches may be nested arbitrarily:

\begin{verbatim}
  case xs of
    { Cons p (Cons q (Cons r Nil)) -> e0
    ; _ -> e1 } -- default case
\end{verbatim}

However, this cannot be directly compiled. Instead, the cases must be
flattened into matching single constructors:

\begin{verbatim}
  case xs of
    { Cons p tmp0 ->
      case tmp0 of
        { Cons q tmp1 ->
          case tmp1 of
            { Cons r tmp2 ->
              case tmp2 of
                { Nil -> e0
                ; _   -> e1 }
            ; _ -> e1 }
        ; _ -> e1 }
    ; _ -> e1 }
\end{verbatim}

For simplicity, my IR will require a default case in every pattern
match. If not provided in source, this will be automatically generated
as an expression that throws an error.

\subsection{Objectives}

\begin{itemize}

  \item Create a parser for a Haskell-like functional programming syntax into an AST

  \item Implement inference and checking for the Hindley-Milner type system with ADTs
  using Algorithm W

  \item Convert this AST into a simpler, untyped IR language

  \item Convert this IR into AMD64 assembly (which supports lazy evaluation with
    thunks), using a low-level runtime written in C

  \item Implement a garbage collector to prevent memory leaks in written code

\end{itemize}

\section{Design}

\subsection{High-level overview}

The compiler pipeline will have approximately the following stages:

\begin{itemize}
  \item CLI argument parsing: parse all the arguments given to the
    compiler, and set its state as specified.

  \item Code parsing: for each input file, parse the file's
    declarations into an AST, reporting errors if parsing fails.

  \item Type checking: Run the Algorithm W based type checker over each
    binding, reporting any type errors that occur.

  \item IR generation: Convert the AST to a simplified intermediate
    representation, flattening let bindings and performing other
    simplification steps.

  \item Code generation: Convert the IR into AMD64 assembly source.

  \item Assembly: Assemble the asm code into object files.

  \item Linking: Link all the compiled object files together alongside
    the runtime code.
\end{itemize}

Most of this functionality will be implemented in the compiler itself,
with two exceptions. Firstly, converting AMD64 assembly to machine code
is a complex process due to the highly complex and backwards-compatible
instruction encoding; therefore, I will use external utilities - likely
the \verb'nasm' assembler - to perform this stage. Secondly, linking
objects together is out of scope for this project, requiring in-depth
knowledge of the object format (ELF64 on Linux or PE on Windows) and
more. Therefore, I will also use external tools for this by calling into
the system linker \verb'ld'.

\subsection{CLI}

As with most compilers, my program will use a command-line interface.
The interface will take a series of flags followed by the names of the
files to compile; the specific usage will be defined by the following
manual page.

\begin{verbatim}

SYNOPSIS
  qntc [-S|-c] [-v] [-static|-shared] [-On] [-o outfile] infile...

DESCRIPTION
  qntc reads the quanta source files specified by 'infile' and compiles them
  into a binary specified by 'outfile'.

  By default, qntc will compile the files specified on the command-line by
  'infile' to an executable. This executable will be named as specified by
  'outfile' or, if 'outfile' is not specified, a default name based on the
  platform ('a.out' on Linux; 'out.exe' on Windows).

  Flags '-S' and '-c' change the type of file the compiler outputs.  Normally,
  the compiler performs compilation, assembly, and linking; however, if given
  '-S', the compiler will not assemble, and if given '-c', the compiler will
  assemble each file, but will not link them together.

  qntc uses 'nasm' and 'ld' internally for compiling asm to object files and
  linking object files into binaries. If 'nasm' is not in PATH, and '-S' is not
  given, qntc will return an error. If 'ld' is not in PATH, and neither '-S' nor
  '-c' is given, qntc will return an error.

OPTIONS
  -S          Compile, but do not assemble; output asm files. The created files
              will be in amd64 NASM syntax. If 'outfile' is not specified and
              there are multiple input files, the output filenames are the input
              filenames with '.asm' concatenated on the end. If 'outfile' is
              specified and there are multiple input files, the output filenames
              are the input filenames with '.outfile' concatenated on the end.
              If 'outfile' is not specified and there is a single input file,
              the output file is 'out.asm'. If 'outfile' is specified and there
              is a single output file, the output filename is 'outfile'.

  -c          Compile and assemble, but do not link; output object files. This
              is subject to the same naming rules as '-S', except the '.o' file
              extension is used in place of '.asm'.

  -v          Enables verbose mode. In this mode, the compiler will output
              timestamped information about every stage of compilation. This
              exists primarily for compiler debugging, but may be useful
              elsewhere.

  -static     (default) Enables the use of static linkage for the produced
              binary. This has no effect if '-S' or '-c' is given.

  -shared     Enables the use of shared (dynamic) linkage for the produced
              binary. This has no effect if '-S' or '-c' is given.

  -On         Set the optimisation level to 'n'. 'n' may be any integer from 0
              to 9 inclusive; it specifies how aggressively optimisations should
              be performed, where 0 means "perform no optimisations" and 9 means
              "perform all optimisations". Note that higher optimisation levels
              are not guaranteed to have any effect.

  -o outfile  Set the output filename.

\end{verbatim}

\subsection{Algorithm W}

Algorithm W is a relatively simple algorithm for Hindley-Milner type
inference/checking. It was the original algorithm used for
Hindley-Milner type inference - described by Milner in 1978 - and has
stood the test of time as being relatively easy to implement whilst also
efficient enough for reasonable use.

An implementation of the HM type system has three important data types:
\verb'Type', \verb'TypeScheme' and \verb'Kind'. Types are the types
we're familiar with; they contain some type names, possibly with
application for polymorphic types like \verb'List'. However, this raises
an important point: we need to disallow objects with types like
\verb'List', as these are type constructors, not inhabited types. The
logic to this system is described by the ``kind'' of a type. Kinds are
effectively types of types; the kind \verb'*', or \verb'Star', is the
kind of an inhabited type (one that can have values, e.g. \verb'Int' and
\verb'List String'), and kinds can also be made with an arrow - i.e.
\verb'a -> b' is the kind of a type which takes a type argument of kind
\verb'a' and gives back a type of kind \verb'b'. The ability of a type
to have kinds other than \verb'Star' is referred to as ``higher-kinded
types''; this is an important feature of most functional languages.
Lastly, a type scheme is a description of a polymorphic type; it is a
type alongside a list for universally quantified (i.e. polymorphic) type
variables. These data types are defined in Haskell as follows:

\begin{verbatim}

  data Type
    = TConc Text   -- Conc meaning 'concrete' - i.e. a specific type
    | TVar Integer
    | TApp Type Type

  data TypeScheme
    = TypeScheme (Set Integer) Type

  data Kind
    = KStar
    | KArrow Kind Kind

\end{verbatim}

My definition of \verb'Type' will be slightly different to this. As I
want to support user-supplied type signatures with arbitrary names for
polymorphic variables, I plan to make \verb'TVar' store a \verb'Text',
and add another constructor \verb'TUnif' for unification variables (the
formal name for type variables used during type checking / inference):

\begin{verbatim}

  data Type
    = TConc Text
    | TVar Text
    | TUnif Integer
    | TApp Type Type

\end{verbatim}

Note that as a function type \verb'a -> b' is merely syntactic sugar for
\verb'(->) a b', functions \verb'a -> b' are represented as
\verb'TApp (TApp (TConc "->") a) b'.

Algorithm W works by generating unification variables to stand for
unknown types, and, as values are used in contexts that limit their
type, generating a ``substitution'' - via a process called
``unification'' - which describes more specific ways of representing a
unification variable. It can be described using a set of type rules.

\subsubsection{Typing rules}

TODO

\subsection{Runtime}

I have previously mentioned the language's runtime. This will be a piece
of code written in C (and potentially some assembly) which contains its
initialization code and garbage collector, as well as any other common
routines that turn out to be neccesary in compiled code. This will be
stored in either an object file or a static library file, and
automatically linked against the generated objects by the compiler.

\subsection{In-memory object representation}

An important consideration for a compiled language is how objects will
be represented in-memory at runtime. One of the advantages of static
type checking is that unlike languages like Python, all type information
can be erased at runtime; this slightly decreases memory usage, and
speeds up the code as types do not have to be checked during execution.

An object (closure) can be modelled as a header, followed by some number
of 1-word (8-byte) values, some of which are pointers. The header
contains three main things. The first is a number describing the type of
the object. This does not refer to the type in the context of the source
language, but rather whether the object represents a thunk, a data
constructor, or a function. The second thing the header contains is a
record of how many pointers and non-pointers are in the object; this is
so that the garbage collector can correctly follow and identify
references and move objects. Lastly, the header contains a pointer to
the object's ``entry code''. The meaning of this differs depending on
the type of the object. For thunks, this entry code is the code that
will evaluate the thunk to weak head normal form. For functions, the
entry code is the code that will run the function. For constructors,
this code does not have a clear interpretation; omitting it is
technically possible, and would save a small amount of memory, however
complicates the object structure. Therefore, I will keep the field
present; it will simply contain some meaningless value, like a null
pointer. The values in the object's main body vary depending on the type
of closure it is:

\begin{itemize}
  \item For thunks, the body simply contains two pointers: a pointer to
    the function object to be called, and a pointer to the object which
    is its argument.

  \item For ADT instances, the body contains a non-pointer encoding
    which constructor was used, and some number of pointers referencing
    the ADT's parameters.

  \item For primitive data types (e.g. \verb'Nat'), the body simply
    contains a non-pointer encoding the value. These primitives can be
    thought of as ADTs with a very large number of constructors and no
    extra parameters.

  \item For functions, the body contains some number (possibly zero) of
    pointers to values referenced by the function's closure.
\end{itemize}

There are a few interesting details here. For example, thunks which will
evaluate to a data constructor need to be sufficiently sized to hold the
largest possible constructor of that type so that evaluation of the
thunk does not clobber other values on the heap. However, for the most
part, the object representation is relatively simple.

\end{document}
